Cost function looks like summation(predicted-actual)^2. The sum is small when network predicts accurately but it is large when network doesn't know what it is doing. So the cost function defines how bad is your network and biases are.
Start with any point and move when the slope is positive move to left and move right when it is negative. Doing this repeatively you acn reach to the some local minimum function. Depends on at which location we started we have many minimal function. Your step size should be proportional to the slope, so when you are reaching to the minima the step size are going to smaller and smaller. Compute the negative gradient of cost function which tells us which weights should increase or decrease in-order to reach the global minima and these steps repeat it again and again. So that you can reach the minimum value of the function.    